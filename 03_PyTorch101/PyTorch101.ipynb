{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch101.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyajitghana/TSAI-DeepVision-EVA4.0/blob/master/03_PyTorch101/PyTorch101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kYM3ylzYo1Q",
        "colab_type": "text"
      },
      "source": [
        "![PyTorch](https://devblogs.nvidia.com/wp-content/uploads/2017/04/pytorch-logo-dark.png)\n",
        "\n",
        "An open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcVqSTrWY6oz",
        "colab_type": "text"
      },
      "source": [
        "# Tensor - Pytorch's core data structure\n",
        "\n",
        "In Python we can create lists, lists of lists, lists of lists and so on. In NumPy there is a `numpy.ndarray` which represents `n`- dimensional array. In math there is a special name for the generalization of vectors and matrices to a higher dimensional space - a tensor\n",
        "\n",
        "Tensor is an entity with a defined number of dimensions called an order (rank). \n",
        "\n",
        "**Scalar** can be considered as a rank-0-tensor. \n",
        "\n",
        "**Vector** can be introduced as a rank-1-tensor. \n",
        "\n",
        "**Matrices** can be considered as a rank-2-tensor.\n",
        "\n",
        "# Tensor Basics\n",
        "\n",
        "Let's import the torch module first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-oFVL2tYYqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGxuR5IEaFJa",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Creation\n",
        "Let's view examples of matrices and tensors generation\n",
        "\n",
        "2-dimensional (rank-2) tensor of zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "897JQc25aD3E",
        "colab_type": "code",
        "outputId": "58464d4a-9020-4cd4-c0d9-b484f3dbeb1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "torch.zeros(3, 4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pujmpEBhaPRA",
        "colab_type": "text"
      },
      "source": [
        "Random rank-3 tensor:\n",
        "_read the print below and convince yourself how this is a rank-3-tensor and learn what those 2, 3, 4 values are there for_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBrznTNhaOL7",
        "colab_type": "code",
        "outputId": "6f1ec0e3-f2b8-4b6e-d689-4a523ae0609c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "torch.rand(2, 3, 4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.1496, 0.1955, 0.0168, 0.8581],\n",
              "         [0.8465, 0.9719, 0.3727, 0.3965],\n",
              "         [0.1888, 0.9655, 0.1059, 0.6187]],\n",
              "\n",
              "        [[0.8315, 0.2324, 0.1961, 0.2771],\n",
              "         [0.7985, 0.4999, 0.2379, 0.6684],\n",
              "         [0.4301, 0.3964, 0.8896, 0.4072]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO4fIr15asdi",
        "colab_type": "text"
      },
      "source": [
        "I am hoping you have noticed 4-elements in a row, 3 rows making one block and there are 2 blocks. \n",
        "\n",
        "Random rank-4-tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiCLvMGCaVZ3",
        "colab_type": "code",
        "outputId": "c7ebbec3-c6af-444c-d08e-ef2ae00e0f72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "torch.rand(2, 2, 2, 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.6224, 0.1067, 0.8029],\n",
              "          [0.8310, 0.2495, 0.8430]],\n",
              "\n",
              "         [[0.1353, 0.1070, 0.4053],\n",
              "          [0.3400, 0.1287, 0.1927]]],\n",
              "\n",
              "\n",
              "        [[[0.2149, 0.3825, 0.2153],\n",
              "          [0.8078, 0.9697, 0.4213]],\n",
              "\n",
              "         [[0.3739, 0.0086, 0.1753],\n",
              "          [0.0626, 0.8847, 0.3811]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF6VL68s3AMV",
        "colab_type": "text"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "How many dimensions are there in a tensor defined as below?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IbXcI3x3Ibt",
        "colab_type": "code",
        "outputId": "8c1d460a-bba4-4c1d-fd64-daf7fd0e13db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "a = torch.rand(1, 1, 1, 1)\n",
        "print(a)\n",
        "print(a.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[0.9795]]]])\n",
            "torch.Size([1, 1, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4omDSGobWpr",
        "colab_type": "text"
      },
      "source": [
        "## Answer : 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmgpqssRa9jF",
        "colab_type": "text"
      },
      "source": [
        ".\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "There are many more ways to create tensor using some restrictions on values it should contatn - for the full reference, please follow the [official docs](https://pytorch.org/docs/stable/torch.html#creation-ops). \n",
        "\n",
        "\n",
        ".\n",
        "---\n",
        "\n",
        "\n",
        "# Python / NumPy / Pytorch interoperability\n",
        "\n",
        "You can create tensors from python as well as numpy arrays. You can also convert torch tensors to numpy arrays. So, the interoperability between torch and numpy is pretty good. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipgpeWPfa6gE",
        "colab_type": "code",
        "outputId": "abd1cd50-f4bb-4710-e38b-bab4985403be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Simple Python List\n",
        "python_list = [1, 2]\n",
        "\n",
        "# Create a numpy array from python list\n",
        "numpy_array = np.array(python_list)\n",
        "\n",
        "# Create a torch Tensor from python list\n",
        "tensor_from_list = torch.tensor(python_list)\n",
        "\n",
        "# Create a torch Tensor from Numpy array\n",
        "tensor_from_array = torch.tensor(numpy_array)\n",
        "\n",
        "# Another way to create a torch Tensor from Numpy array (share same storage)\n",
        "tensor_from_array_v2 = torch.from_numpy(numpy_array)\n",
        "\n",
        "# Convert torch tensor to numpy array\n",
        "array_from_tensor = tensor_from_array.numpy()\n",
        "\n",
        "print('List:   ', python_list)\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_list)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Tensor: ', tensor_from_array_v2)\n",
        "print('Array:  ', array_from_tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List:    [1, 2]\n",
            "Array:   [1 2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Array:   [1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_x-86B8gqik",
        "colab_type": "text"
      },
      "source": [
        "**Difference between** `torch.Tensor` **and** `torch.from_numpy`\n",
        "\n",
        "Pytorch aims to be an effective library for computations. What does it mean? It means that pytorch avoids memory copying if it can. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAZvjS8fsKmN",
        "colab_type": "text"
      },
      "source": [
        "**torch.tensor always copies data** \\\n",
        "**torch.from_numpy always shares data from the actual numpy array**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHCKDGpygn_d",
        "colab_type": "code",
        "outputId": "0a6f2d7a-22f3-4e19-b84e-9f42c05b8575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "numpy_array[0] = 10\n",
        "\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Tensor: ', tensor_from_array_v2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array:   [10  2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([10,  2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9b9Wb-Ofq4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_numpy = np.array([1, 2])\n",
        "a_tensor = torch.tensor(a_numpy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CKtdNRM3SMp",
        "colab_type": "text"
      },
      "source": [
        "## Question 2:\n",
        "\n",
        "Assume that we moved our complete (cats vs dogs) image dataset to numpy arrays. Then we use torch.from_numpy to convert these images to tensor. Then we apply a specific data augmentation strategy called \"CutOut\" which blocks a portion of the image directly on these tensors. What will happen to the accuracy of a model trained on this strategy compared to the one without this strategy? CutOut strategy is shown below: \n",
        "\n",
        "![CutOut](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSnSyN835AmtQPKQbPjDHX-FmshNilbtexX95cRGQPwl56QCGDn)\n",
        "\n",
        "## Question 3:\n",
        "Why do you think we are observing this behavior?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1MATjhokSNs",
        "colab_type": "text"
      },
      "source": [
        "## Answer 2\n",
        "\n",
        "CutOut is a image augmentation technique like dropout, but here we remove a spatial region of the image, another difference is that the cutout is performed to the input of the network, rather than at the intermediate layers like in dropout.\n",
        "\n",
        "referring to [1], the addition of cutout to the\n",
        "ResNet18 and WRN-28-10 models increased their accuracy\n",
        "on CIFAR-10 and CIFAR-100 by between 0.4 to 2.0 percentage points. We draw attention to the fact that cutout\n",
        "yields these performance improvements even when applied\n",
        "to complex models that already utilize batch normalization,\n",
        "dropout, and data augmentation. Adding cutout to the current state-of-the-art shake-shake regularization models improves performance by 0.3 and 0.6 percentage points on\n",
        "CIFAR-10 and CIFAR-100 respectively, yielding new stateof-the-art results of 2.56% and 15.20% test error.\n",
        "\n",
        "![cutout](https://github.com/satyajitghana/TSAI-DeepVision-EVA4.0/blob/master/03_PyTorch101/cutout.jpg?raw=true)\n",
        "\n",
        "[1] DeVries, T. and Taylor, G.W., 2017. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552.\n",
        "\n",
        "## Answer 3\n",
        "\n",
        "Intuitively, how i imagine this is that we are forcing the network to not memorize some specific deatails of the image and making it look into different parts of the image to make the prediction, this will make the network learn more general features, In 100-class classification, the model needs more fine-grained features to separate between the classes.\n",
        "\n",
        "Cutout Regularization should force a CNN model to develop a more diverse set of features for classifying images. Instead of just focusing on the wheels of a car, Cutout Regularization should force the model to look at other details of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZq7O-aihSOA",
        "colab_type": "text"
      },
      "source": [
        "We have two different ways to create tensor from its NumPy counterpart - one copies memory and another one shares the same underlying storage. It works in the opposite way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNFOwV8EhPwQ",
        "colab_type": "code",
        "outputId": "e59a8479-c1e6-4e64-a5cc-c1a6be5e3847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "array_from_tensor = tensor_from_array.numpy()\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor)\n",
        "\n",
        "tensor_from_array[0] = 11\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor:  tensor([1, 2])\n",
            "Array:  [1 2]\n",
            "Tensor:  tensor([11,  2])\n",
            "Array:  [11  2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVepu-ALKYwv",
        "colab_type": "text"
      },
      "source": [
        "If you make changes in the ``.numpy()`` then its not reflected in the tensor, but any changes in the tensor is reflected in ``.numpy()``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM9ytbvKhtCw",
        "colab_type": "text"
      },
      "source": [
        "## Data types\n",
        "\n",
        "The basic data type of all Deep Learning-related operations is float, but sometimes you may need something else. Pytorch support different number types for its tensors the same way NumPy does it - by specifying the data type on tensor creation or via casting. Ths full list of supported data types can be found [here](https://pytorch.org/docs/stable/tensors.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd6WkzJ4hpYi",
        "colab_type": "code",
        "outputId": "10dffaeb-c23a-4e4a-8459-f62c0a419843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "tensor = torch.zeros(2, 2)\n",
        "print('Tensor with default type: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.float16)\n",
        "print('Tensor with 16-bit float: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.int16)\n",
        "print('Tensor with integers: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.bool)\n",
        "print('Tensor with boolean data: ', tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor with default type:  tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "Tensor with 16-bit float:  tensor([[0., 0.],\n",
            "        [0., 0.]], dtype=torch.float16)\n",
            "Tensor with integers:  tensor([[0, 0],\n",
            "        [0, 0]], dtype=torch.int16)\n",
            "Tensor with boolean data:  tensor([[False, False],\n",
            "        [False, False]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9F4Dkdr40TE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 4:\n",
        "We saw above that some times numpy and tensors share same storage and changing one changes the other. \n",
        "If we define a rank-2-tensor with ones (dtype of f16), and then convert it into a numpy data type using tensor.numpy() and store it in a variable called \"num\", and then we perform this operation `num = num * 0.5`, will the original tensor have 1.0s or 0.5s as its element values? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vsPMK0fJH8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4b332d1c-c077-4741-c4a2-4e681feaac5d"
      },
      "source": [
        "tensor = torch.ones(2, 2, dtype=torch.float16)\n",
        "num = tensor.numpy()\n",
        "print('Num : ', num)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1.]\n",
            " [1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mI8Oq4JJ6JE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "82d979c4-c298-4497-d339-ed9ae9d51cf2"
      },
      "source": [
        "num = num * 0.5;\n",
        "print('Num : ', num)\n",
        "print('Tensor: ', tensor)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num :  [[0.5 0.5]\n",
            " [0.5 0.5]]\n",
            "Tensor:  tensor([[1., 1.],\n",
            "        [1., 1.]], dtype=torch.float16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiQt8Mmm51OE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 5: \n",
        "If the operation `num = num*5` is changed to `num[:] = num*5` will the original tensor have 1.0s or 0.5s as its element values? \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5bL0WDqKlSy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c3d2a8a2-d8b6-4d11-9d37-42dfe5f56535"
      },
      "source": [
        "tensor = torch.ones(2, 2, dtype=torch.float16)\n",
        "num = tensor.numpy()\n",
        "print('Num : ', num)\n",
        "print('Tensor : ', tensor)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num :  [[1. 1.]\n",
            " [1. 1.]]\n",
            "Tensor :  tensor([[1., 1.],\n",
            "        [1., 1.]], dtype=torch.float16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ZSF8foKuUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8aeafdc0-0027-4159-806b-881a4e5e2076"
      },
      "source": [
        "num[:] = num * 5\n",
        "print('Num : ', num)\n",
        "print('Tensor : ', tensor)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num :  [[5. 5.]\n",
            " [5. 5.]]\n",
            "Tensor :  tensor([[5., 5.],\n",
            "        [5., 5.]], dtype=torch.float16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37nu2nmzK5_M",
        "colab_type": "text"
      },
      "source": [
        "## Answer : if ``num[:] = num * 5`` is used then the original tensor value is also changed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzh8UB8KiVmb",
        "colab_type": "text"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "Tensor provides access to its elements via the same `[]` operation as a regular python list or NumPy array. However, as you may recall from NumPy usage, the full power of math libraries is accessible only via vectorized operations, i.e. operations without explicit looping over all vector elements in python and using implicit optimized loops in C/C++/CUDA/Fortran/etc. available via special function calls. Pytorch employs the same paradigm and provides a wide range of vectorized operations. Let's take a look at some examples. \n",
        "\n",
        "Joining a list of tensors together with `torch.cat`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMaCDKPhiUAb",
        "colab_type": "code",
        "outputId": "4f087bb8-f7c4-49de-ec2b-5289e6969ae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "a = torch.zeros(3, 2)\n",
        "b = torch.ones(3, 2)\n",
        "print(torch.cat((a, b), dim=0))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bj4aeE86zdH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 6: \n",
        "Is the transpose of concatenated a & b tensor on dimension 1, same as the contatenated tensor of a & b on dimension 0? \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7H0IMDYLgO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "71d24ccd-65cf-4904-9f2a-1776928f86a7"
      },
      "source": [
        "print('Along dim=0\\n', torch.cat((a, b), dim=0).t())\n",
        "print('Along dim=1\\n', torch.cat((a, b), dim=1))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Along dim=0\n",
            " tensor([[0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.]])\n",
            "Along dim=1\n",
            " tensor([[0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRcLlwJDMQW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84527911-1d3b-4744-fb64-694e793360bc"
      },
      "source": [
        "print('Shape a', a.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape a torch.Size([3, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TXNne69j7LP",
        "colab_type": "text"
      },
      "source": [
        "Indexing with another tensor/array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiE-Fsi4jVd6",
        "colab_type": "code",
        "outputId": "1a76963d-eaf9-4b8b-fdbe-16c799ebea79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "a = torch.arange(start=0, end=10)\n",
        "indices = np.arange(0, 10) > 5\n",
        "print(a)\n",
        "print(indices)\n",
        "print(a[indices])\n",
        "\n",
        "indices = torch.arange(start=0, end=10) %5\n",
        "print(indices)\n",
        "print(a[indices])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "[False False False False False False  True  True  True  True]\n",
            "tensor([6, 7, 8, 9])\n",
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isP9bA2XOc_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f07b876d-76c0-44ee-c844-b79184cd5dc6"
      },
      "source": [
        "torch.arange(start=0, end=10)[torch.arange(start=0, end=10) % 5]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6q0-eQ0OoIh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "866358c4-1aea-442f-c07c-b7268907188b"
      },
      "source": [
        "torch.arange(start=0, end=10) % 3 "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS4dnlu47WQu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 7:\n",
        "\n",
        "`a` is defined as `torch.arange(start=0, end=10)`. We will create `b` using the two operations as below. In both cases do we get the same value?\n",
        "\n",
        "\n",
        "1.   indices variable created by the modulo operation on arange between 0 and 10. Then a new varialble `b` is created from `a` using the last 5 elements of indices. \n",
        "2.   indices variable created by the modulo operation on arange betwenn 1 and 11. Then a new varialble `b` is created from `a` using the last 5 elements of indices. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHgKDUYbOwc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cf76a2e1-4ec9-462c-dc6c-ae20640f1c8a"
      },
      "source": [
        "a = torch.arange(start=0, end=10)\n",
        "print('a: ', a)\n",
        "indices_1 = torch.arange(start=0, end=10) % 5\n",
        "print('indices_1: ', indices_1)\n",
        "b_1 = a[indices_1[-5:]]\n",
        "print('b_1: ', b_1)\n",
        "indices_2 = torch.arange(start=1, end=11) % 5\n",
        "print('indices_2: ', indices_2)\n",
        "b_2 = a[indices_2[-5:]]\n",
        "print('b_2: ', b_2)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a:  tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "indices_1:  tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
            "b_1:  tensor([0, 1, 2, 3, 4])\n",
            "indices_2:  tensor([1, 2, 3, 4, 0, 1, 2, 3, 4, 0])\n",
            "b_2:  tensor([1, 2, 3, 4, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_CiH7NhQdhQ",
        "colab_type": "text"
      },
      "source": [
        "## Answer: We get the same values but in a different order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ4ZCVsTk-KH",
        "colab_type": "text"
      },
      "source": [
        "What should we do if we have, say, rank-2-tensor and want to select only some rows?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GtRpotjkt1q",
        "colab_type": "code",
        "outputId": "5701b15d-5653-4bb2-c036-36edf8ae547d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "tensor = torch.rand((5, 3))\n",
        "rows = torch.tensor([0, 2])\n",
        "print(tensor)\n",
        "tensor[rows]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9532, 0.9691, 0.4046],\n",
            "        [0.8964, 0.3201, 0.4458],\n",
            "        [0.7439, 0.8556, 0.1922],\n",
            "        [0.3325, 0.7155, 0.4722],\n",
            "        [0.3595, 0.1969, 0.2830]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9532, 0.9691, 0.4046],\n",
              "        [0.7439, 0.8556, 0.1922]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIJnr_N2_Qaf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 8: \n",
        "\n",
        "Consider a tensor defined as `torch.rand((6, 5))`. Is the shape of the new tensor created by taking the 0th, 2nd and 4th row of the old tensor same as the shape of the a newer tensor created by taking the 0th, 2nd and 4th row of the old tensor after transposing it by operation `torch.transpose(tensor, 0, 1)` ?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79wrIoALRc-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "6529db77-ce97-4943-f24d-78657e5da66c"
      },
      "source": [
        "tensor = torch.rand((6, 5))\n",
        "print('tensor: \\n', tensor)\n",
        "print(tensor[[0, 2, 4], :].shape)\n",
        "print(tensor.t()[[0, 2, 4], :].shape)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor: \n",
            " tensor([[0.0143, 0.5887, 0.5579, 0.6328, 0.9807],\n",
            "        [0.9546, 0.8116, 0.3494, 0.7403, 0.8320],\n",
            "        [0.5261, 0.7321, 0.5834, 0.3416, 0.8813],\n",
            "        [0.4674, 0.2309, 0.4393, 0.1253, 0.6280],\n",
            "        [0.2876, 0.3694, 0.3017, 0.5511, 0.6623],\n",
            "        [0.9799, 0.5432, 0.5164, 0.4824, 0.7782]])\n",
            "torch.Size([3, 5])\n",
            "torch.Size([3, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naDzFMkslU0b",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Shapes\n",
        "\n",
        "Reshaping a tensor is a frequently used operation. We can change the shape of a tensor without the memory copying overhead. There are two methods for that: `reshape` and `view`. \n",
        "\n",
        "The difference is the following: \n",
        "\n",
        "\n",
        "*   view tries to return a tensor, and it shares the same memory with the original tensor. In case, if it cannot reuse the same memory due to [some reason](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view), it just fails. \n",
        "*   reshape always returns the tensor with the desired shape and tries to reuse the memory. If it cannot, it creates a copy\n",
        "\n",
        "Let's see with the help of an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HClDkqLLlMJh",
        "colab_type": "code",
        "outputId": "58fc7b31-a6e3-4149-dd50-b67fd32e0b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "tensor = torch.rand(2, 3, 4)\n",
        "print('Pointer to data: ', tensor.data_ptr())\n",
        "print('Shape: ', tensor.shape)\n",
        "\n",
        "reshaped = tensor.reshape(24)\n",
        "\n",
        "view = tensor.view(3, 2, 4)\n",
        "print('Reshaped tensor - pointer to data', reshaped.data_ptr())\n",
        "print('Reshaped tensor shape ', reshaped.shape)\n",
        "\n",
        "print('Viewed tensor - pointer to data', view.data_ptr())\n",
        "print('Viewed tensor shape ', view.shape)\n",
        "\n",
        "assert tensor.data_ptr() == view.data_ptr()\n",
        "\n",
        "assert np.all(np.equal(tensor.numpy().flat, reshaped.numpy().flat))\n",
        "\n",
        "print('Original stride: ', tensor.stride())\n",
        "print('Reshaped stride: ', reshaped.stride())\n",
        "print('Viewed stride: ', view.stride())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pointer to data:  85181952\n",
            "Shape:  torch.Size([2, 3, 4])\n",
            "Reshaped tensor - pointer to data 85181952\n",
            "Reshaped tensor shape  torch.Size([24])\n",
            "Viewed tensor - pointer to data 85181952\n",
            "Viewed tensor shape  torch.Size([3, 2, 4])\n",
            "Original stride:  (12, 4, 1)\n",
            "Reshaped stride:  (1,)\n",
            "Viewed stride:  (8, 4, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIN5jSppm4yC",
        "colab_type": "text"
      },
      "source": [
        "The basic rule about reshaping the tensor is definitely that you cannot change the total number of elements in it, so the product of all tensor's dimensions should always be the same. It gives us the ability to avoid specifying one dimension when reshaping the tensor - Pytorch can calculate it for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3D19ERFmzOl",
        "colab_type": "code",
        "outputId": "b49f6393-7a79-4da7-bb31-b5d8cd885e3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(tensor.reshape(3, 2, 4).shape)\n",
        "print(tensor.reshape(3, 2, -1).shape)\n",
        "print(tensor.reshape(3, -1, 4).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2, 4])\n",
            "torch.Size([3, 2, 4])\n",
            "torch.Size([3, 2, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObgCQKUiETak",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 9:\n",
        "\n",
        "Consider a tensor `a` created with [1, 2, 3] and [1, 2, 3] of size (2, 3) is reshaped with operation `.reshape(-1, 2)`. Also consider a tensor `b` created with [[2, 1]] and of size (1, 2), later operated with `view(2, -1)` operation. \n",
        "\n",
        "If we do a dot product of a and b (using `torch.mm`) and perform the sum of all the elements (using `torch.sum`) what do we get? (enter int value without any decimal point in the quiz)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYaaUPE5XdZX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "52de52f5-c078-4b2b-f9e6-9a7804fb3ae0"
      },
      "source": [
        "a = torch.tensor([[1, 2, 3], [1, 2, 3]])\n",
        "print('a shape: ', a.shape)\n",
        "a_reshaped = a.reshape(-1, 2)\n",
        "b = torch.tensor([[2, 1]])\n",
        "print('b shape: ', b.shape)\n",
        "b_viewed = b.view(2, -1)\n",
        "print('sum: ', torch.sum(torch.mm(a_reshaped, b_viewed)))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a shape:  torch.Size([2, 3])\n",
            "b shape:  torch.Size([1, 2])\n",
            "sum:  tensor(18)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekUdvKcoYoui",
        "colab_type": "text"
      },
      "source": [
        "## Answer: 18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mza7QPg3ndeV",
        "colab_type": "text"
      },
      "source": [
        "**Alternative ways to view tensors** - `expand` or `expand_as`.\n",
        "\n",
        "\n",
        "\n",
        "*   `expand` - requires the desired shape as an input\n",
        "*   `expand_as` - uses the shape of another tensor\n",
        "\n",
        "These operations \"repeat\" tensor's values along the specified axes without actually copying the data. \n",
        "\n",
        "As the documentation says, expand:\n",
        "\n",
        "\n",
        "> returns a new view of the self tensor with singleton dimensions expanded to a larger size. Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1. \n",
        "\n",
        "**Use case:**\n",
        "\n",
        "\n",
        "\n",
        "*   index multi-channel tensor with single-channel mask - imagine a color image with 3 channels (RGB) and binary mask for the area of interest on that image. We cannot index the image with this kind of mask directly since the dimensions are different, but we can use `expand_as` operation to create a view of the mask that has the same dimensions as the image we want to apply it to, but has not copied the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz33E-V7nPQT",
        "colab_type": "code",
        "outputId": "8bcfc71f-b0bb-4c37-e5e0-5c4ffa38aacb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Create a black image\n",
        "image = torch.zeros(size=(3, 256, 256), dtype=torch.int)\n",
        "\n",
        "# Leave the borders and make the rest of the image Green\n",
        "image[1, 18:256 - 18, 18:256 - 18] = 255\n",
        "\n",
        "# Create a mask of the same size\n",
        "mask = torch.zeros(size=(256, 256), dtype=torch.bool)\n",
        "\n",
        "# Assuming the green region in the original image is the Region of interest, change the mask to white for that area\n",
        "mask[18:256 - 18, 18:256 - 18] = 1\n",
        "\n",
        "# Create a view of the mask with the same dimensions as the original image\n",
        "mask_expanded = mask.expand_as(image)\n",
        "print(mask_expanded.shape)\n",
        "\n",
        "mask_np = mask_expanded.numpy().transpose(1, 2, 0) * 255\n",
        "image_np = image.numpy().transpose(1, 2, 0)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[0, mask] += 128\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[mask_expanded] += 128\n",
        "image.clamp_(0, 255)\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 256, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiXBx0k3ptOI",
        "colab_type": "text"
      },
      "source": [
        "In the example above, one can also find a couple of useful tricks:\n",
        "\n",
        "\n",
        "*   `clamp` method and function is a Pytorch's analogue of NumPy's `clip` function\n",
        "*   many operations on tensors have in-place form, that does not return modified data, but change values in the tensor. The in-place version of the operation has trailing underscore according to Pytorch's naming convension - in the exmaple above it is `clamp_`\n",
        "*   tensors have the same indexing as Numpy's arrays - one can use `:` seperated range, negative indexes and so on.\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Images and their representations\n",
        "\n",
        "Now, let's discuss images, their representations and how different Python librarties work with them. \n",
        "\n",
        "Probably, the most well-known library for image loading and simple processing is [Pillow](https://pillow.readthedocs.io/en/stable/). \n",
        "\n",
        "However, many people in deep learning area stick with OpenCV for image loading and processing with some usage of another libraries when it is justified by performance/functionality. This is because OpenCV is in general much faster than the other libraries. Here you can find a couple of benchmarks: \n",
        "\n",
        "*   https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading\n",
        "*   https://github.com/albumentations-team/albumentations#benchmarking-results\n",
        "\n",
        "To sum up the benchmarks above, there are two most common image formats, PNG and JPEGs. If your data is in PNG format - use OpenCV to read it. If it is in JPEG - use libturbojpeg. For image processing, use OpenCV if possible. _We will be using PIL a lot along with these._\n",
        "\n",
        "As you will read the code from others, you may find out that some of them use Pillow/something else to read data. You should know, that color image representations in OpenCV and other libraries are different - OpenCV uses \"BGR\" channel order, while others use \"RGB\" one. \n",
        "\n",
        "To change \"BRG\" <-> \"RGB\" the only thing we need to do it to change channel order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYv4sZMmpndu",
        "colab_type": "code",
        "outputId": "1b6e97b2-94f7-4880-b92f-2077b2a58d9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "URL = 'https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRCA40ftnscVzfV8ft8e7vIzQXfXeZdtco8nknJrfCUW6INI40U'\n",
        "\n",
        "urlretrieve(URL, 'mars.jpg')\n",
        "\n",
        "bgr_image = cv2.imread('mars.jpg') \n",
        "# remember to add your own image in case you run this block, if you want to use the same image, \n",
        "# download it from: https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRCA40ftnscVzfV8ft8e7vIzQXfXeZdtco8nknJrfCUW6INI40U\n",
        "rgb_image = bgr_image[..., ::-1]\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(bgr_image)\n",
        "ax[1].imshow(rgb_image)\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAACHCAYAAADtJRlTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoi4TeNBGHle",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 10:\n",
        "\n",
        "Looking at the results above it can be said that the pixel values in the blue channels would be very small compared to red channel. True/False?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZJH3lMUZ5Rx",
        "colab_type": "text"
      },
      "source": [
        "## Answer: Why ? ``::-1`` will only reverse the channel order, the values of the respective channels remain unchanged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ugFd57zNwa",
        "colab_type": "text"
      },
      "source": [
        "# Autograd\n",
        "\n",
        "Pytorch supports automatic differentiation. The module which implements this is called **AutoGrad**. It calculates the gradients and keeps track in forward and backward passes. For primitive tensors, you need to enable or disable it using the `required_grad` flag. But, for advanced tensors, it is enabled by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMOp4aiou6JR",
        "colab_type": "code",
        "outputId": "9b67f16a-48fb-441a-ac5c-d5216682c90b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad = True)\n",
        "print(a)\n",
        "result = a * 5\n",
        "print(result)\n",
        "\n",
        "# grad can be implicitly created only for scalar outputs\n",
        "# so let's calculate the sum here so that the output becomes a scalar and we can apply a backward pass\n",
        "mean_result = result.sum()\n",
        "print(mean_result)\n",
        "# calculate gradient\n",
        "mean_result.backward()\n",
        "# print gradient of a\n",
        "print(a.grad)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2476, 0.4312, 0.4864, 0.3625, 0.3507],\n",
            "        [0.4258, 0.3856, 0.4559, 0.2613, 0.3028],\n",
            "        [0.1312, 0.8620, 0.9031, 0.1292, 0.3792]], requires_grad=True)\n",
            "tensor([[1.2380, 2.1559, 2.4322, 1.8126, 1.7537],\n",
            "        [2.1291, 1.9282, 2.2794, 1.3063, 1.5139],\n",
            "        [0.6560, 4.3099, 4.5157, 0.6459, 1.8959]], grad_fn=<MulBackward0>)\n",
            "tensor(30.5727, grad_fn=<SumBackward0>)\n",
            "tensor([[5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym0Amk2IGfLx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 11: \n",
        "\n",
        "Why the gradient of a is all 5s above?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzD_vaclaI7L",
        "colab_type": "text"
      },
      "source": [
        "## Answer: The derivative of ``a * 5`` w.r.t ``a`` is ``5``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PDgGq2R0k7I",
        "colab_type": "text"
      },
      "source": [
        "As we see, Pytorch automagically calculated the gradient value for us. It looks to be the correct value - we multiplied an input by 5, so the gradient of this operation equals to 5.\n",
        "\n",
        "# Disabling Autograd for tensors\n",
        "\n",
        "We don't need to compute gradients for all the variables that are involved in the pipeline. The Pytorch API provides 2 ways to disable autograd.\n",
        "\n",
        "`detach` - returns a copy of the tensor with autograd disabled. This \n",
        "\n",
        "1.   copy is built on the same memory as the original tensor, so in-place size / stride / storage changes (such as resize_ / resizeas / set / transpose) modifications are not allowed.\n",
        "2.   torch.no_grad() - It is a context manager that allows you to guard a series of operations from autograd without creating new tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqVG9fQb0cLW",
        "colab_type": "code",
        "outputId": "2081f403-31a2-4296-c670-b899e246a8cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "detached_a = a.detach()\n",
        "detached_result = detached_a * 5\n",
        "result = a * 10\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "mean_result.backward()\n",
        "a.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqpch2Be02J7",
        "colab_type": "code",
        "outputId": "0a5ee8e8-81b8-46f0-c7fc-ac406657e928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    detached_result = a * 5\n",
        "result = a * 10\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "mean_result.backward()\n",
        "a.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjh2rYOPJUAZ",
        "colab_type": "text"
      },
      "source": [
        "# Custom Network\n",
        "\n",
        "A fully-connected ReLU network with one hidden layer and no biases, trained to predict y from x by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation uses PyTorch tensors to manually compute the forward pass, loss, and backward pass.\n",
        "\n",
        "A PyTorch Tensor is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation.\n",
        "\n",
        "The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU, just cast the Tensor to a cuda datatype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nf5RaB104Vp",
        "colab_type": "code",
        "outputId": "f1666d3a-7fe4-4f62-eac3-4d4ec49f4319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y\n",
        "    h = x.mm(w1)\n",
        "    h_relu = h.clamp(min=0)\n",
        "    y_pred = h_relu.mm(w2)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2*(y_pred - y)\n",
        "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "    grad_h = grad_h_relu.clone()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 31758600.0\n",
            "1 32050022.0\n",
            "2 36072572.0\n",
            "3 37410324.0\n",
            "4 31568128.0\n",
            "5 20429292.0\n",
            "6 10572478.0\n",
            "7 4985997.0\n",
            "8 2526845.75\n",
            "9 1508727.25\n",
            "10 1054520.625\n",
            "11 815929.0625\n",
            "12 666264.0625\n",
            "13 558850.625\n",
            "14 475591.125\n",
            "15 408389.15625\n",
            "16 352946.96875\n",
            "17 306584.78125\n",
            "18 267514.625\n",
            "19 234385.28125\n",
            "20 206101.859375\n",
            "21 181857.765625\n",
            "22 161005.75\n",
            "23 142976.359375\n",
            "24 127311.375\n",
            "25 113640.640625\n",
            "26 101690.4453125\n",
            "27 91206.84375\n",
            "28 81981.4609375\n",
            "29 73838.03125\n",
            "30 66639.8984375\n",
            "31 60263.7734375\n",
            "32 54595.15234375\n",
            "33 49545.8359375\n",
            "34 45030.44140625\n",
            "35 40987.3828125\n",
            "36 37358.83984375\n",
            "37 34095.984375\n",
            "38 31159.744140625\n",
            "39 28509.73828125\n",
            "40 26114.974609375\n",
            "41 23946.939453125\n",
            "42 21981.232421875\n",
            "43 20196.203125\n",
            "44 18573.849609375\n",
            "45 17097.775390625\n",
            "46 15753.689453125\n",
            "47 14529.3369140625\n",
            "48 13410.81640625\n",
            "49 12387.5654296875\n",
            "50 11450.43359375\n",
            "51 10591.32421875\n",
            "52 9803.2490234375\n",
            "53 9080.0908203125\n",
            "54 8416.830078125\n",
            "55 7806.86865234375\n",
            "56 7245.150390625\n",
            "57 6727.541015625\n",
            "58 6251.466796875\n",
            "59 5812.02392578125\n",
            "60 5406.404296875\n",
            "61 5031.40771484375\n",
            "62 4684.52001953125\n",
            "63 4363.5751953125\n",
            "64 4066.315185546875\n",
            "65 3790.91748046875\n",
            "66 3535.6435546875\n",
            "67 3298.8330078125\n",
            "68 3079.0546875\n",
            "69 2874.973876953125\n",
            "70 2685.53857421875\n",
            "71 2509.5849609375\n",
            "72 2346.01171875\n",
            "73 2193.716796875\n",
            "74 2052.00732421875\n",
            "75 1920.1109619140625\n",
            "76 1797.206787109375\n",
            "77 1682.9014892578125\n",
            "78 1576.350830078125\n",
            "79 1476.9986572265625\n",
            "80 1384.2802734375\n",
            "81 1297.7130126953125\n",
            "82 1216.881103515625\n",
            "83 1141.4337158203125\n",
            "84 1070.9173583984375\n",
            "85 1004.998046875\n",
            "86 943.3328857421875\n",
            "87 885.6656494140625\n",
            "88 831.7174682617188\n",
            "89 781.21337890625\n",
            "90 733.9569091796875\n",
            "91 689.6867065429688\n",
            "92 648.2241821289062\n",
            "93 609.3890380859375\n",
            "94 573.0238647460938\n",
            "95 538.9085693359375\n",
            "96 506.932861328125\n",
            "97 476.9517822265625\n",
            "98 448.8245849609375\n",
            "99 422.43115234375\n",
            "100 397.6570129394531\n",
            "101 374.40118408203125\n",
            "102 352.57647705078125\n",
            "103 332.0729675292969\n",
            "104 312.794921875\n",
            "105 294.6974182128906\n",
            "106 277.6921081542969\n",
            "107 261.7049560546875\n",
            "108 246.67868041992188\n",
            "109 232.54379272460938\n",
            "110 219.25083923339844\n",
            "111 206.74549865722656\n",
            "112 194.98545837402344\n",
            "113 183.91986083984375\n",
            "114 173.5099334716797\n",
            "115 163.710205078125\n",
            "116 154.48606872558594\n",
            "117 145.809326171875\n",
            "118 137.6314239501953\n",
            "119 129.928955078125\n",
            "120 122.67034149169922\n",
            "121 115.8333969116211\n",
            "122 109.3941650390625\n",
            "123 103.32122802734375\n",
            "124 97.60002136230469\n",
            "125 92.20671844482422\n",
            "126 87.12101745605469\n",
            "127 82.32450866699219\n",
            "128 77.8026123046875\n",
            "129 73.54269409179688\n",
            "130 69.5175552368164\n",
            "131 65.7207260131836\n",
            "132 62.139686584472656\n",
            "133 58.75823211669922\n",
            "134 55.56679916381836\n",
            "135 52.55545425415039\n",
            "136 49.71220016479492\n",
            "137 47.026283264160156\n",
            "138 44.490718841552734\n",
            "139 42.09501266479492\n",
            "140 39.833560943603516\n",
            "141 37.69744110107422\n",
            "142 35.679161071777344\n",
            "143 33.77341079711914\n",
            "144 31.970470428466797\n",
            "145 30.266590118408203\n",
            "146 28.656946182250977\n",
            "147 27.136035919189453\n",
            "148 25.697052001953125\n",
            "149 24.336406707763672\n",
            "150 23.04935646057129\n",
            "151 21.83376121520996\n",
            "152 20.68268585205078\n",
            "153 19.594892501831055\n",
            "154 18.56589126586914\n",
            "155 17.59193992614746\n",
            "156 16.670595169067383\n",
            "157 15.797794342041016\n",
            "158 14.972750663757324\n",
            "159 14.191768646240234\n",
            "160 13.452658653259277\n",
            "161 12.752931594848633\n",
            "162 12.090636253356934\n",
            "163 11.463327407836914\n",
            "164 10.870047569274902\n",
            "165 10.307950973510742\n",
            "166 9.775832176208496\n",
            "167 9.271023750305176\n",
            "168 8.793429374694824\n",
            "169 8.34087085723877\n",
            "170 7.9118146896362305\n",
            "171 7.505539417266846\n",
            "172 7.121114730834961\n",
            "173 6.756673812866211\n",
            "174 6.411507606506348\n",
            "175 6.083901882171631\n",
            "176 5.773329257965088\n",
            "177 5.47934627532959\n",
            "178 5.2006940841674805\n",
            "179 4.936069965362549\n",
            "180 4.685511112213135\n",
            "181 4.448120594024658\n",
            "182 4.222657680511475\n",
            "183 4.008946418762207\n",
            "184 3.80613374710083\n",
            "185 3.6141958236694336\n",
            "186 3.432015895843506\n",
            "187 3.259380340576172\n",
            "188 3.0951433181762695\n",
            "189 2.939307689666748\n",
            "190 2.792050361633301\n",
            "191 2.6520936489105225\n",
            "192 2.5190298557281494\n",
            "193 2.3929426670074463\n",
            "194 2.2733805179595947\n",
            "195 2.159909725189209\n",
            "196 2.0519416332244873\n",
            "197 1.9497241973876953\n",
            "198 1.8525333404541016\n",
            "199 1.760326623916626\n",
            "200 1.6726136207580566\n",
            "201 1.5896373987197876\n",
            "202 1.510761022567749\n",
            "203 1.4359368085861206\n",
            "204 1.3647959232330322\n",
            "205 1.2971709966659546\n",
            "206 1.2331724166870117\n",
            "207 1.1722056865692139\n",
            "208 1.1143041849136353\n",
            "209 1.0593018531799316\n",
            "210 1.007165551185608\n",
            "211 0.9576284885406494\n",
            "212 0.9105119109153748\n",
            "213 0.8659011721611023\n",
            "214 0.8233097791671753\n",
            "215 0.7828183770179749\n",
            "216 0.7445234060287476\n",
            "217 0.7080211043357849\n",
            "218 0.6733644008636475\n",
            "219 0.6405298709869385\n",
            "220 0.6091402769088745\n",
            "221 0.5794140100479126\n",
            "222 0.5512052178382874\n",
            "223 0.5242210626602173\n",
            "224 0.49872326850891113\n",
            "225 0.4744877517223358\n",
            "226 0.4513762593269348\n",
            "227 0.4293454587459564\n",
            "228 0.4085777997970581\n",
            "229 0.3887757360935211\n",
            "230 0.3699207305908203\n",
            "231 0.3520374000072479\n",
            "232 0.3349713981151581\n",
            "233 0.31872233748435974\n",
            "234 0.3033391535282135\n",
            "235 0.2887325584888458\n",
            "236 0.274750292301178\n",
            "237 0.261523962020874\n",
            "238 0.24888984858989716\n",
            "239 0.2368861436843872\n",
            "240 0.22549009323120117\n",
            "241 0.21466070413589478\n",
            "242 0.2043197602033615\n",
            "243 0.19444860517978668\n",
            "244 0.1851504147052765\n",
            "245 0.17627668380737305\n",
            "246 0.167837992310524\n",
            "247 0.1598074734210968\n",
            "248 0.15210139751434326\n",
            "249 0.1448473334312439\n",
            "250 0.13790667057037354\n",
            "251 0.131365105509758\n",
            "252 0.12508101761341095\n",
            "253 0.11911118775606155\n",
            "254 0.11341140419244766\n",
            "255 0.10800672322511673\n",
            "256 0.1028582751750946\n",
            "257 0.09798503667116165\n",
            "258 0.09332974255084991\n",
            "259 0.08889491856098175\n",
            "260 0.08465629816055298\n",
            "261 0.08064049482345581\n",
            "262 0.07681509107351303\n",
            "263 0.07316432148218155\n",
            "264 0.06966780126094818\n",
            "265 0.06637058407068253\n",
            "266 0.06324397027492523\n",
            "267 0.06024259328842163\n",
            "268 0.05739470571279526\n",
            "269 0.054688483476638794\n",
            "270 0.052119314670562744\n",
            "271 0.04965169355273247\n",
            "272 0.04730745777487755\n",
            "273 0.0450669601559639\n",
            "274 0.04294483736157417\n",
            "275 0.040925972163677216\n",
            "276 0.03900964558124542\n",
            "277 0.03717661276459694\n",
            "278 0.03543967753648758\n",
            "279 0.0337730348110199\n",
            "280 0.03219873085618019\n",
            "281 0.03070288524031639\n",
            "282 0.029257720336318016\n",
            "283 0.027886956930160522\n",
            "284 0.026577722281217575\n",
            "285 0.025330200791358948\n",
            "286 0.02416166663169861\n",
            "287 0.02303498238325119\n",
            "288 0.021967390552163124\n",
            "289 0.020940806716680527\n",
            "290 0.019965315237641335\n",
            "291 0.019039513543248177\n",
            "292 0.018158018589019775\n",
            "293 0.01731853373348713\n",
            "294 0.01651827059686184\n",
            "295 0.01575564034283161\n",
            "296 0.015028577297925949\n",
            "297 0.014338006265461445\n",
            "298 0.01367791648954153\n",
            "299 0.013053777627646923\n",
            "300 0.012456092052161694\n",
            "301 0.011900143697857857\n",
            "302 0.011358257383108139\n",
            "303 0.010838810354471207\n",
            "304 0.010344583541154861\n",
            "305 0.009873605333268642\n",
            "306 0.009427757933735847\n",
            "307 0.009005491621792316\n",
            "308 0.008596297353506088\n",
            "309 0.00821093562990427\n",
            "310 0.00784134678542614\n",
            "311 0.007488586474210024\n",
            "312 0.007157954387366772\n",
            "313 0.006840822286903858\n",
            "314 0.006530621089041233\n",
            "315 0.006245010998100042\n",
            "316 0.00597359100356698\n",
            "317 0.005709121003746986\n",
            "318 0.005459518171846867\n",
            "319 0.005219162441790104\n",
            "320 0.0049895113334059715\n",
            "321 0.004774176049977541\n",
            "322 0.0045637148432433605\n",
            "323 0.004369906149804592\n",
            "324 0.004180947318673134\n",
            "325 0.004004266578704119\n",
            "326 0.0038304156623780727\n",
            "327 0.0036699569318443537\n",
            "328 0.003511784365400672\n",
            "329 0.0033633937127888203\n",
            "330 0.0032207630574703217\n",
            "331 0.0030863129068166018\n",
            "332 0.0029570702463388443\n",
            "333 0.002833931241184473\n",
            "334 0.0027149608358740807\n",
            "335 0.00260185613296926\n",
            "336 0.0024958536960184574\n",
            "337 0.0023958482779562473\n",
            "338 0.002299679210409522\n",
            "339 0.0022093611769378185\n",
            "340 0.002123106736689806\n",
            "341 0.0020385333336889744\n",
            "342 0.0019590354058891535\n",
            "343 0.0018837382085621357\n",
            "344 0.001809535315260291\n",
            "345 0.001738927559927106\n",
            "346 0.0016724476590752602\n",
            "347 0.0016076626488938928\n",
            "348 0.0015481500886380672\n",
            "349 0.0014905532589182258\n",
            "350 0.00143438670784235\n",
            "351 0.0013805835042148829\n",
            "352 0.0013314008247107267\n",
            "353 0.001282932236790657\n",
            "354 0.0012383435387164354\n",
            "355 0.0011926456354558468\n",
            "356 0.0011488988529890776\n",
            "357 0.001107676886022091\n",
            "358 0.0010685905581340194\n",
            "359 0.0010312163503840566\n",
            "360 0.0009958610171452165\n",
            "361 0.0009608397958800197\n",
            "362 0.0009283794788643718\n",
            "363 0.0008963500149548054\n",
            "364 0.0008664828492328525\n",
            "365 0.0008362384978681803\n",
            "366 0.000809919903986156\n",
            "367 0.0007832592818886042\n",
            "368 0.0007574578048661351\n",
            "369 0.0007320985896512866\n",
            "370 0.0007091775769367814\n",
            "371 0.0006864184397272766\n",
            "372 0.0006625175010412931\n",
            "373 0.000643000123091042\n",
            "374 0.0006233592866919935\n",
            "375 0.000603619497269392\n",
            "376 0.0005845965933986008\n",
            "377 0.000567750888876617\n",
            "378 0.0005505040753632784\n",
            "379 0.0005353392334654927\n",
            "380 0.0005186341004446149\n",
            "381 0.0005042094853706658\n",
            "382 0.0004898385959677398\n",
            "383 0.0004750003572553396\n",
            "384 0.00046157476026564837\n",
            "385 0.0004486998077481985\n",
            "386 0.00043694564374163747\n",
            "387 0.0004240899870637804\n",
            "388 0.0004125413834117353\n",
            "389 0.00040177488699555397\n",
            "390 0.00039010526961646974\n",
            "391 0.0003795214870478958\n",
            "392 0.00036935642128810287\n",
            "393 0.0003599391784518957\n",
            "394 0.000349317560903728\n",
            "395 0.00034008940565399826\n",
            "396 0.0003316097136121243\n",
            "397 0.00032426955294795334\n",
            "398 0.00031549317645840347\n",
            "399 0.0003073900588788092\n",
            "400 0.00029880512738600373\n",
            "401 0.0002915926743298769\n",
            "402 0.0002843897673301399\n",
            "403 0.0002776698675006628\n",
            "404 0.00027084239991381764\n",
            "405 0.0002636458957567811\n",
            "406 0.0002572024241089821\n",
            "407 0.0002506673918105662\n",
            "408 0.0002446374564897269\n",
            "409 0.00023931576288305223\n",
            "410 0.00023301375040318817\n",
            "411 0.00022780349536333233\n",
            "412 0.00022280965640675277\n",
            "413 0.00021761089737992734\n",
            "414 0.00021342521358747035\n",
            "415 0.0002087651810143143\n",
            "416 0.00020389934070408344\n",
            "417 0.00019920938939321786\n",
            "418 0.00019474804867058992\n",
            "419 0.00019090541172772646\n",
            "420 0.00018678118067327887\n",
            "421 0.00018264786922372878\n",
            "422 0.00017851643497124314\n",
            "423 0.00017509638564661145\n",
            "424 0.00017138823750428855\n",
            "425 0.00016742563457228243\n",
            "426 0.00016408544615842402\n",
            "427 0.0001609554747119546\n",
            "428 0.00015719677321612835\n",
            "429 0.00015448722115252167\n",
            "430 0.00015162747877184302\n",
            "431 0.00014867442951072007\n",
            "432 0.0001455424790037796\n",
            "433 0.00014256990107242018\n",
            "434 0.0001402330381097272\n",
            "435 0.00013754006067756563\n",
            "436 0.00013488087279256433\n",
            "437 0.0001323916221735999\n",
            "438 0.00012977779260836542\n",
            "439 0.00012690506991930306\n",
            "440 0.00012498193245846778\n",
            "441 0.0001223488216055557\n",
            "442 0.00012001789582427591\n",
            "443 0.00011772381549235433\n",
            "444 0.00011562852159840986\n",
            "445 0.00011356182221788913\n",
            "446 0.00011171475489391014\n",
            "447 0.00010968480637529865\n",
            "448 0.00010769476648420095\n",
            "449 0.00010583687981124967\n",
            "450 0.0001038017580867745\n",
            "451 0.00010229118197457865\n",
            "452 0.00010049084085039794\n",
            "453 9.858112025540322e-05\n",
            "454 9.715757914818823e-05\n",
            "455 9.533525008009747e-05\n",
            "456 9.367675374960527e-05\n",
            "457 9.229543502442539e-05\n",
            "458 9.092890104511753e-05\n",
            "459 8.937315578805283e-05\n",
            "460 8.776551112532616e-05\n",
            "461 8.650664676679298e-05\n",
            "462 8.493308996548876e-05\n",
            "463 8.383347449125722e-05\n",
            "464 8.213740511564538e-05\n",
            "465 8.084993169177324e-05\n",
            "466 7.958157220855355e-05\n",
            "467 7.842940976843238e-05\n",
            "468 7.718612323515117e-05\n",
            "469 7.610400643898174e-05\n",
            "470 7.490513962693512e-05\n",
            "471 7.400309550575912e-05\n",
            "472 7.274068047991022e-05\n",
            "473 7.196483784355223e-05\n",
            "474 7.084853859851137e-05\n",
            "475 6.992815178819001e-05\n",
            "476 6.860346911707893e-05\n",
            "477 6.76293857395649e-05\n",
            "478 6.662376836175099e-05\n",
            "479 6.561855116160586e-05\n",
            "480 6.48303102934733e-05\n",
            "481 6.384638254530728e-05\n",
            "482 6.297523941611871e-05\n",
            "483 6.213611777639017e-05\n",
            "484 6.12086005276069e-05\n",
            "485 6.019724969519302e-05\n",
            "486 5.96832724113483e-05\n",
            "487 5.8864476159214973e-05\n",
            "488 5.824784966534935e-05\n",
            "489 5.718426109524444e-05\n",
            "490 5.6477991165593266e-05\n",
            "491 5.575194882112555e-05\n",
            "492 5.4827942221891135e-05\n",
            "493 5.420942761702463e-05\n",
            "494 5.359261922421865e-05\n",
            "495 5.300668271956965e-05\n",
            "496 5.2347757446113974e-05\n",
            "497 5.173077806830406e-05\n",
            "498 5.1051662012469023e-05\n",
            "499 5.037110531702638e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycwxAPHZLNST",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Question 12\n",
        "\n",
        "In the code above, why do we have 2 in '2.0*(y_pred - y)`?\n",
        "\n",
        "## Question 13\n",
        "In the code above, what does `grad_h[h < 0] = 0` signify?\n",
        "\n",
        "## Question 14\n",
        "In the code above, how many \"epochs\" have we trained the model for? \n",
        "\n",
        "## Question 15\n",
        "In the code above, if we take the trained model, and run it on fresh  inputs, the trained model will be able to predict fresh output with high accuracy. \n",
        "\n",
        "## Question 16\n",
        "In the code above, if we dont use clone in `grad_h = grad_h_relu.clone()` the model will still train without any issues. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5PBVvMroJzz",
        "colab_type": "text"
      },
      "source": [
        "## Asnwer 12: Derivative of ``(y_pred-y)**2`` is ``2*(y_pred-y) \n",
        "## Answer 13: Derivative of ReLU for ``x < 0`` is ``0`` and for ``x > 0`` is ``1``\n",
        "## Answer 14: 500 epoch\n",
        "## Answer 15: \n",
        "## Answer 16:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnWkOtnGoGrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}